{
  "metadata": {
    "name": "ecem_weblog_scala",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%sh\nls /zeppelin/ecem"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkConf\nimport org.apache.spark.sql.SparkSession"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval rdd \u003dsc.textFile(\"/zeppelin/ecem/weblog-main/test/*.log\")\nrdd.take(1)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval spark \u003d SparkSession\n  .builder()\n  .appName(\"Spark weblog proje\")\n  .config(\"spark.some.config.option\", \"some-value\")\n  .getOrCreate()"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nrdd.take(5).foreach(println)"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nrdd.collect().take(5).foreach(a \u003d\u003e {\n    a.foreach(e \u003d\u003e print(e ))\n    println()\n  })\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nrdd.count()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Create an RDD containing only those lines that are requests for JPG files. Use the filter operation with a transformation function that takes a string RDD element and returns a boolean value"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval rdd_user_id \u003d rdd.map( line \u003d\u003e line.split(\u0027 \u0027){2})\nval rdd_user_idd \u003d spark.sparkContext.parallelize(rdd_user_id.collect())\nrdd_user_idd.map(line \u003d\u003e println(line))\nprint(rdd_user_idd.take(10).foreach(println))\n\nval dff_id \u003d rdd_user_idd.map(fields \u003d\u003e fields{0})\nval df_id \u003drdd_user_idd.map( x \u003d\u003e x).toDF(\"user_id\")\nprint(df_id.show(5))"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval rdd_ip_address \u003d rdd.map( line \u003d\u003e line.split(\u0027 \u0027){0})\nval rdd_ip_addresss \u003d spark.sparkContext.parallelize(rdd_ip_address.collect())\nrdd_ip_addresss.map(line \u003d\u003e println(line))\nprint(rdd_ip_addresss.take(10).foreach(println))\n\nval dff_ip \u003d rdd_ip_address.map(fields \u003d\u003e fields{0})\nval df_ip \u003drdd_ip_address.map( x \u003d\u003e x).toDF(\"user_ip\")\nprint(df_ip.show(5))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval rdd_request \u003d rdd.map( line \u003d\u003e line.split(\u0027\"\u0027){1})\nval rdd_requestt \u003d spark.sparkContext.parallelize(rdd_request.collect())\nrdd_requestt.map(line \u003d\u003e println(line))\nprint(rdd_requestt.take(10).foreach(println))\n\nval dff_request \u003d rdd_request.map(fields \u003d\u003e fields{0})\nval df_request \u003drdd_request.map( x \u003d\u003e x).toDF(\"request\")\nprint(df_request.show(5))"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval jpglogsdf \u003d df_ip.join( df_id).join(df_request)"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "/// Detected implicit cartesian product for INNER join between logical plans , so need to conf this"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nspark.conf.set(\"spark.sql.crossJoin.enabled\", \"true\")"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nprint(jpglogsdf.show(5))"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval jpglogsRDD \u003d jpglogsdf.rdd"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\njpglogsRDD.take(5).foreach(println)"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nprintln(\"class: \" + jpglogsRDD.getClass)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval filtered_rdd \u003d jpglogsRDD.filter( line \u003d\u003e line.toString.contains(\"GET\"))\nfiltered_rdd.take(5)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval rdd_length \u003d rdd.map( x \u003d\u003e x.toString.length ) \nrdd_length.take(10)"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nrdd_length.take(5).foreach(println)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval FieldsRDD\u003drdd.map(line \u003d\u003e line.split(\" \"))\nFieldsRDD.take(5)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nfor line in FieldsRDD.take(5):\n    for field in line:\n        print(field)\n    print(\"---------------\") "
    }
  ]
}