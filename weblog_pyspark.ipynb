{
  "metadata": {
    "name": "ecem_weblog_pyspark",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%sh\nls"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%sh\nmkdir ecem\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%sh\nls /zeppelin/ecem/"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%sh\nwget https://github.com/ecemce/weblog/archive/refs/heads/main.zip -P /zeppelin/ecem/\n\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%sh\nls  /zeppelin/ecem"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%sh\nunzip /zeppelin/ecem/main.zip -d /zeppelin/ecem\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%sh\nls /zeppelin/ecem/weblog-main/test/\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\nfrom pyspark import SparkContext , SparkConf\nfrom pyspark.sql import SparkSession\nfrom pyspark.rdd import RDD"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\nspark \u003d SparkSession.builder.getOrCreate()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\nsc\u003dSparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// In spark, create an RDD from the uploaded web logs data files in the /yourname/weblogs/ directory in HDFS."
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\ndata_file \u003d \"/zeppelin/ecem/weblog-main/test/*.log\"\nrdd \u003d sc.textFile(data_file)\nrdd.take(1)\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\ntype(rdd)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\nfor line in rdd.take(2):\n    print(line)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\nrdd.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "/// Create an RDD containing only those lines that are requests for JPG files. Use the filter operation with a transformation function that takes a string RDD element and returns a boolean value"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\nrdd_user_id \u003d rdd.map(lambda line : line.split(\u0027 \u0027)[2] )\nrdd_user_id \u003d spark.sparkContext.parallelize(rdd_user_id.collect())\nprint(type(rdd_user_id))\nprint(rdd_user_id.take(10))\n\ndf_id \u003d rdd_user_id.map(lambda fields: float(fields[0]))\ndf_id\u003drdd_user_id.map(lambda x: (x, )).toDF([\"user_id\"])\nprint(df_id.show(5))"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\nrdd_ip_address \u003d rdd.map(lambda line : line.split(\u0027 \u0027)[0])\nrdd_ip_address \u003d spark.sparkContext.parallelize(rdd_ip_address.collect())\nprint(type(rdd_ip_address))\nprint(rdd_ip_address.take(10))\n\ndf_ip \u003d rdd_ip_address.map(lambda fields: float(fields[0]))\ndf_ip\u003drdd_ip_address.map(lambda x: (x, )).toDF([\"ip_address\"])\nprint(df_ip.show(5))"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\nrdd_request \u003d rdd.map(lambda line : line.split(\u0027\"\u0027)[1])\nrdd_request \u003d spark.sparkContext.parallelize(rdd_request.collect())\nprint(type(rdd_request))\nprint(rdd_request.take(10))\n\ndf_request \u003d rdd_request.map(lambda fields: string(fields[0]))\ndf_request\u003drdd_request.map(lambda x: (x, )).toDF([\"request\"])\nprint(df_request.show(5))"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\njpglogsdf \u003d df_ip.join( df_id).join(df_request)"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "/// Detected implicit cartesian product for INNER join between logical plans , so need to conf this\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\nspark.conf.set(\"spark.sql.crossJoin.enabled\", \"true\")"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\njpglogsdf.show(5)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\njpglogsRDD \u003d jpglogsdf.rdd.map(list)\n\nfor line in jpglogsRDD.take(5):\n    print(line)\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\ntype(jpglogsRDD)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\na\u003djpglogsRDD.filter(lambda line: \u0027GET\u0027 in line )\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\nrdd_length \u003d rdd.map(lambda x: len(x))\nrdd_length.collect()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\nfor line in rdd_length.take(5):\n    print(line)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\nrdd_length.take(5)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\nFieldsRDD\u003drdd.map(lambda line: line.split(\" \"))\nFieldsRDD.take(5)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\nfor line in FieldsRDD.take(5):\n    for field in line:\n        print(field)\n    print(\"---------------\") "
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}